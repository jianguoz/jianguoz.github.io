<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="shortcut icon" href="./Jianguo Zhang_files/myIcon.ico">

<meta name="keywords" content="Jianguo Zhang, CS, SE, ZJU, Zhejiang University"> 
<meta name="description" content="Jianguo Zhang&#39;s Homepage">
<link rel="stylesheet" href="./Jianguo Zhang_files/jemdoc.css" type="text/css">
<title>Jianguo Zhang</title>
<script type="text/javascript" async="" src="./Jianguo Zhang_files/ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99347283-1', 'auto');
  ga('send', 'pageview');

</script>
<script type="text/javascript" src="./Jianguo Zhang_files/jquery-1.12.4.min.js"></script></head>
<body>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Jianguo Zhang Âº†Âª∫ÂõΩ</h1><h1>
				</h1></div>

				<h3>Ph.D in Computer Science</h3>
				<p>
					University of Illinois at Chicago (UIC),<br>
					Chicago, IL, U.S. <br>
					<br>
					Email: jianguozhang@salesforce.com 
					(<u>Note:</u> jzhan51@uic.edu is deactivated by UIC)
				</p>
				<p>
					<a href="https://github.com/jianguoz" target="_blank"><img src="./Jianguo Zhang_files/items/Github.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://scholar.google.com/citations?user=mAAVFEsAAAAJ&hl=en&oi=ao" target="_blank"><img src="./Jianguo Zhang_files/items/google_scholar.png" height="30px" style="margin-bottom:-3px"></a> 
 					<a href="https://twitter.com/JianguoZhang3" target="_blank"><img src="./Jianguo Zhang_files/items/Twitter_logo2013.png" height="30px" style="margin-bottom:-3px"></a> 	
				</p>
			</td>
			<td>
				<img src="./Jianguo Zhang_files/photos/personal_latest.jpg" border="0" width="240"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--  <h2>Biography <!--[<a href="./Jianguo Zhang_files/files/Jianguo_Zhang.pdf" target="_blank">Resume</a>]--></h2>

I am a Senior Research Scientist at <a href="https://www.salesforceairesearch.com/">Salesforce AI Research</a> and a recipient of the <b> Annual T&P (Technology and Product) All-Star Awards</b>, awarded to the top 1% with approximately 150 winners across all Technology. I earned my Ph.D. in Computer Science from the University of Illinois at Chicago, where I was advised by Prof. <a href="https://scholar.google.com/citations?user=D0lL1r0AAAAJ&hl=en">Philip S. Yu</a> in the <a href="https://bdsc-uic.github.io/index.html">BDSC Lab</a>. My current research primarily focuses on AI Agents and Conversational AI. Specifically, my work involves:
<li> Large-scale and advanced data collection, processing, synthesis, verification, and more.</li>
<li> Training large action models and designing AI agents.</li>
<li> Creating end-to-end dialogue systems, with an emphasis on accurate natural language understanding and dialogue state tracking without complicated pipelines.</li>
	
<br><br>
	<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Rancho&effect=ice|fire-animation|neon|splintered|grass">

<h2>News</h2>  
<!-- 	<li><b> May 2022</b>: ‚ù§Ô∏è Welcome to maintain <a href="https://github.com/jianguoz/BiWeekly-Research-Paper-Series">(Bi) Weekly-Research-Paper-Series</a> together!  -->
	
<li><b>Aug 20 2025</b>: üéâüéâüéâ: <a href="https://github.com/SalesforceAIResearch/xLAM/tree/main/actionstudio">ActionStudio</a> (A Lightweight Framework for Data and Training of Large Action Models) and <a href="https://latte-web.github.io/">LATTE</a> (Learning to think with vision specialists) are both accepted by EMNLP 2025 Main conference! 
<li><b>Aug 05 2025</b>:  üí´ <a href="https://github.com/SalesforceAIResearch/xLAM/tree/main/actionstudio">ActionStudio</a> has been updated with new features, improved training configuration tracking, and general code enhancements!</li>
	
<li><b>Apr 2025</b>: üèÜ Check out <a href="https://gorilla.cs.berkeley.edu/leaderboard.html">xLAM-2-fc-r</a>, which achieved <b>Top-1 performance</b> on the latest BFCL Leaderboard!</li>

<li><b>Apr 2025</b>: üöÄ <a href="https://arxiv.org/abs/2503.22673">ActionStudio</a> is now fully open-source! Dive into the details in our <a href="https://arxiv.org/abs/2503.22673">paper</a> and explore the <a href="ActionStudio_README.md">codebase</a> for end-to-end agent orchestration.</li>

<li><b>Apr 2025</b>: üì¢ Discover <a href="https://arxiv.org/pdf/2504.03601">APIGen-MT</a>, our newly open-sourced framework for multi-turn data generation. Learn more on the <a href="https://apigen-mt.github.io/">project website</a>!</li>

	<li><b>Sep 2024</b>: üèÜ Check our latest work  <a href="https://arxiv.org/abs/2409.03215">xLAM: A Family of Large Action Models to Empower AI Agent Systems</a>, which demonstrates top-tier performance across multiple challenging benchmarks and scenarios. Explore more through the <a href="https://t.co/iBAp3MgXGo">xLAM GitHub Repo</a>, <a href="https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4">HuggingFace Model Collections</a> and <a href="https://www.salesforceairesearch.com/projects/xlam-large-action-models">Project Webpage</a>. 
	</li> 
	<li><b>July 2024</b>: Happy to contribute to the work  <a href="https://www.dataprovenance.org/consent-in-crisis-paper">Consent in Crisis: The Rapid Decline of the AI Data Commons</a>. See also the <a href="https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html">New York Times</a> and <a href="https://x.com/JianguoZhang3/status/1814310784903909532">Twitter</a>. 
	</li> 
	<li><b>June 2024</b>: Check our new work  <a href="https://apigen-pipeline.github.io/">APIGen</a>, the best open-sourced models for function calling. Our dataset <a href="https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k">xlam-function-calling-60k</a>  is currently among the Top-2 trending datasets on HuggingFace, standing out in a field of 176,238 datasets as of July 11, 2024. See also the <a href="https://x.com/Benioff/status/1808365628551844186">Twitter by Salesforce CEO</a>, <a href="https://venturebeat.com/ai/salesforce-proves-less-is-more-xlam-1b-tiny-giant-beats-bigger-ai-models/">VentureBeat</a> and <a href="https://mp.weixin.qq.com/s/B3gyaGwzlQaUXyI8n7Rguw">Êñ∞Êô∫ÂÖÉ</a>. 
	</li> 
	<li><b>March 2024</b>: <a href="https://github.com/SalesforceAIResearch/xLAM">xLAM Repository</a> and <a href="https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4">xLAM v0.1 Model</a> are released! Try it together with  <a href="https://github.com/SalesforceAIResearch/AgentLite/tree/main/benchmark">AgentLite benchmark</a> or other benchamrks, which is comparable to GPT-4! See also the <a href="https://arxiv.org/pdf/2402.15506">Paper</a> and <a href="https://x.com/CaimingXiong/status/1769778308659708316">Twitter by Salesforce AI Research VP</a>. 
	</li> 
	<li><b>Dec 2023</b>:  <a href="https://github.com/salesforce/DialogStudio">DialogStudio Repository</a> has garnered 400 stars.  <a href="https://huggingface.co/datasets/Salesforce/dialogstudio">DialogStudio Data</a> has more than 55k downloads and was previously ranked among the Top-10 trending datasets! See also the  <a href="https://arxiv.org/pdf/2307.10172">Paper</a> and <a href="https://x.com/Yampeleg/status/1683012386381242368">Twitter</a>. 
	</li> 
	<li><b>Jan 2022</b>: I will join Salesforce AI Research as a Research Scientist.
	</li> 
	<li><b>Sep 2021</b>: Finished my internship at <a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a>. The project focuses on few-shot end-to-end task-oriented dialgue systems.  
	</li> 
	<li><b>Aug 2021</b>: Two paper are accepted by <font color=#527bbd><a href="https://2021.emnlp.org/">EMNLP 2021</a></font> main conference as short papers. One first-author work collaborates with Adobe Research and it focuses on natural language understanding. 
	</li> 
	<li><b>June 2021</b>: Check our new released <font color=#527bbd><a href="https://arxiv.org/abs/2106.04564">paper</a></font>  and public resources for <font color=#527bbd><a href="https://github.com/jianguoz/Few-Shot-Intent-Detection">few-shot intent detection</a></font>  w/ and w/o out-of-scope queries. 
<!-- 		I will join <font color=#527bbd><a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a></font> as a research intern.  -->
	</li> 
	<li><b>May 2021</b>: I finished my internship at <font color=#527bbd><a href="https://research.adobe.com/">Adobe Research</a></font> and submitted one research paper, where I also discover PhotoShop user intents and improve intent detection performance using large-scale unlabeled PhotoShop user queries. 
	</li> 
	<li><b>Dec 2020</b>: Our toolkit <font color=#527bbd><a href="https://github.com/CGCL-codes/naturalcc">NaturalCC</a></font> has been released in GitHub, which can be accessed via the <font color=#527bbd><a href="https://xcodemind.github.io/">Homepage</a></font>.
	</li>
	<li><b>Nov 2020</b>: We have released the <font color=#527bbd><a href="https://arxiv.org/abs/2010.13009">paper</a></font>, <font color=#527bbd><a href="https://slideslive.com/38939288/discriminative-nearest-neighbor-fewshot-intent-detection-by-transferring-natural-language-inference">video</a></font> for the EMNLP 2020 paper, and the corresponding code is available <font color=#527bbd><a href="https://github.com/salesforce/DNNC-few-shot-intent">here</a></font>!
	</li>
	<li><b>Sep 2020</b>: One first-author work collaborated with Salesforce Research is accepted by <font color=#527bbd><a href="https://sites.google.com/view/starsem2020/">*SEM 2020</a></font>  as a long paper. The work focuses on multi-domain dialog state tracking and it originally got borderline scores on ACL 2020. We have updated the <font color=#527bbd><a href="https://arxiv.org/pdf/1910.03544.pdf">paper</a></font>!
	</li>
	<li><b>Sep 2020</b>: One first-author work collaborated with Salesforce Research is accepted by <font color=#527bbd><a href="https://2020.emnlp.org/">EMNLP 2020</a></font> main conference as a long paper. The work focuses on natural language understanding. 
	</li>
	<li><b>June 2020</b>: One work collaborated with Google Research is accepted by <font color=#527bbd><a href="https://sites.google.com/view/2ndnlp4convai/">ACL 2020 NLP4ConvAI</a></font>, and the modified high-quality  version of <font color=#527bbd><a href="https://www.aclweb.org/anthology/2020.nlp4convai-1.13.pdf">MultiWOZ 2.2 dataset</a></font> with additional annotation
corrections and state tracking baselines is released  <font color=#527bbd><a href="https://github.com/budzianowski/multiwoz">here</a></font>!
	</li>
	
<!-- <h2>Preprints</h2>
<!-- <li>
	<b>Few-shot End-to-End Task-Oriented Dialogue Systems with Database</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Zhang et al.</b> 
        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [Paper] -->
<!-- </li>
	<li>
	<b>Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Yao Wan, Ye Liu, Caiming Xiong, Philip Yu</font>
<!-- 	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2021.eacl.org/">EACL 2021</a></font>  -->
	<!-- <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2106.04564">[Paper]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/jianguoz/Few-Shot-Intent-Detection" >Resources</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> 
</li>  -->

<h2>Open Sources</h2> 
<!-- 	<li> -->
    <p> <a href="https://github.com/SalesforceAIResearch/xLAM">xLAM</a> | <a href="https://github.com/SalesforceAIResearch/AgentLite/tree/main">AgentLite</a> |  <a href="https://github.com/salesforce/DialogStudio">DialogStudio</a> | <a href="https://github.com/budzianowski/multiwoz">MultiWOZ</a> |<a href="https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection">Data Provenance</a> | <a href="https://github.com/CGCL-codes/naturalcc">NaturalCC</a> </p>
<!-- </li> -->

<h2>Publications</h2>
<!-- <p><b>Last Update: Aug 2025. See more on <a href="https://scholar.google.com">Google Scholar</a> or <a href="https://scholar.google.com">Semantic Scholar</a></b></p> -->
<p><b>Last Update: Aug. 2024</b></p>

<ul>
<li>
	<b>ActionStudio: A Lightweight Framework for Data and Training of Large Action Models</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  <b>Jianguo Zhang</b><font color=#808080 >, Thai Hoang, Ming Zhu, Zuxin Liu, Shiyu Wang, Tulika Awalgaonkar, Akshara Prabhakar, Haolin Chen, Weiran Yao, Zhiwei Liu, Juntao Tan, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2025.emnlp.org/"> EMNLP 2025 Main Conference</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2503.22673">[Paper]</a></font>  /<font color=#527bbd><a href="https://github.com/SalesforceAIResearch/xLAM/tree/main">[Code]</a></font> 
</li>	
<li>
	<b>LATTE: LeArning to Think wiTh Vision SpEcialists</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zixian Ma, <b>Jianguo Zhang</b><font color=#808080 >, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, Ranjay Krishna, Silvio Savarese</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2025.emnlp.org/">üèÜ EMNLP 2025 Main Conference & Featured by Top Media </a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2412.05479">[Paper]</a></font>  /<font color=#527bbd><a href="https://latte-web.github.io/">[Project Website]</a></font> /<font color=#527bbd><a href="https://huggingface.co/collections/Salesforce/cota-datasets-675333e57dd34a4adc5f3ff4">[Data]</a></font> /<font color=#527bbd><a href="https://github.com/SalesforceAIResearch/LATTE">[Code]</a></font> 
</li>	

<li>
	<b>MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zhiwei Liu, Jielin Qiu, Shiyu Wang, <b>Jianguo Zhang</b><font color=#808080 >, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2507.12806">[Paper]</a></font>  /<font color=#527bbd><a href="https://github.com/SalesforceAIResearch/MCPEval">[Code]</a></font> 
</li>	

<li>
	<b>Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Akshara Prabhakar, Zuxin Liu, Ming Zhu, <b>Jianguo Zhang</b><font color=#808080 >, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2504.03601">[Paper]</a></font>  /<font color=#527bbd><a href="https://apigen-mt.github.io/">[Project Website]</a></font> / <font color=#527bbd><a href="https://huggingface.co/datasets/Salesforce/APIGen-MT-5k">[Data]</a></font> 
</li>	
	
<li>
	<b>Bridging the Data Provenance Gap Across Text, Speech and Video</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Shayne Longpre et al. <b>Jianguo Zhang</b><font color=#808080 > (Top Contributor) </font>
		<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://iclr.cc/">üèÜ ICLR 2025 & Featured by Top Media </a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2412.17847">[Paper]</a></font> / <font color=#527bbd><a href="https://www.dataprovenance.org/publications">[Project Website]</a></font> / <font color=#527bbd><a href="https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection">[GitHub]</a></font> 
</li>
<li>	
	<b>xLAM: A Family of Large Action Models to Empower AI Agent Systems</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Awalgaonkar, Rithesh Murthy, Eric Hu, Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong</font>
				<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2025.naacl.org/">üèÜ NAACL 2025 Oral & Featured by Salesforce CEO & Top Media</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2409.03215">[Paper]</a></font> /<font color=#527bbd><a href="https://github.com/SalesforceAIResearch/xLAM">[GitHub]</a></font> /<font color=#527bbd><a href="https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4">[Models]</a></font> 
</li>	

<li>
	<b>PRACT: Optimizing Principled Reasoning and Acting of LLM Agent</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zhiwei Liu, Weiran Yao, <b>Jianguo Zhang</b><font color=#808080 >, Rithesh Murthy, Liangwei Yang, Zuxin Liu, Tian Lan, Ming Zhu, Juntao Tan, Shirley Kokane, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong</font>
				<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://conll.org/2024">CoNLL 2024</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://aclanthology.org/2024.conll-1.33/">[Paper]</a></font> 
</li>	
<li>
	<b>Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zuxin Liu, Thai Hoang, <b>Jianguo Zhang</b><font color=#808080 >, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, Caiming Xiong</font>
		<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://neurips.cc/">  üèÜ NeurIPS 2024 Datasets and Benchmarks Track & Featured by Top Media</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2406.18518">[Paper]</a></font> / <font color=#527bbd><a href="https://apigen-pipeline.github.io/">[Project Website]</a></font> / <font color=#527bbd><a href="https://github.com/SalesforceAIResearch/xLAM">[GitHub]</a></font> /<font color=#527bbd><a href="https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k">[Data]</a></font> 
</li>	
<li>
	<b>Consent in Crisis: The Rapid Decline of the AI Data Commons</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Shayne Longpre et al. <b>Jianguo Zhang</b><font color=#808080 > (Contributor) </font>
		<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://neurips.cc/">üèÜ NeurIPS 2024 Datasets and Benchmarks Track & Featured by Top Media </a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2407.14933">[Paper]</a></font> / <font color=#527bbd><a href="https://www.dataprovenance.org/consent-in-crisis-paper/">[Project Website]</a></font> / <font color=#527bbd><a href="https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection">[GitHub]</a></font> 
</li>	

<li>
	<b>MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin Desai, Jason Wu, Ran Xu, Sarah Tan, <b>Jianguo Zhang</b><font color=#808080 >, Zhiwei Liu, Shirley Kokane, Zuxin Liu, Ming Zhu, Huan Wang, Caiming Xiong, Silvio Savarese</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2406.10290">[ArXiv Paper]</a></font> / <font color=#527bbd><a href="https://github.com/SalesforceAIResearch/MobileAIBench">[GitHub]</a></font> 
</li>	

<li>
	<b>AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zhiwei Liu, Weiran Yao, <b>Jianguo Zhang</b><font color=#808080 >, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, Shelby Heinecke, Caiming Xiong, Silvio Savarese</font>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="">üèÜ Databricks Data + AI Summit</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2402.15538">[Paper]</a></font> / <font color=#527bbd><a href="https://github.com/salesforceairesearch/agentlite">[GitHub]</a></font> 
</li>	

<li>
	<b>AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong</font>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="">üèÜ Hugging Face Daily Papers</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2402.15506">[Paper]</a></font> / <font color=#527bbd><a href="https://github.com/SalesforceAIResearch/xLAM">[GitHub]</a></font>  / <font color=#527bbd><a href="https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4">[Models]</a></font> 
</li>	

<li>
	<b>DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Yu Wang, Zhiwei Liu,  <b>Jianguo Zhang</b><font color=#808080 >, Weiran Yao, Shelby Heinecke, Philip S. Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2312.11336.pdf">[ArXiv Paper]</a></font> 
</li>	

<li>
	<b>Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Yao Wan, Yang He, Zhangqian Bi, <b>Jianguo Zhang</b><font color=#808080 >, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin, Philip S. Yu.</font>
		<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://dl.acm.org/journal/csur">ACM Computing Surveys, 2024</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://dl.acm.org/doi/pdf/10.1145/3664597">[Paper]</a></font> / <font color=#527bbd><a href="https://xcodemind.github.io/">[Project Website]</a></font> 
</li>	

<li>
	<b>BOLAA: Benchmarking and orchestrating LLM-augmented autonomous agents</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zhiwei Liu, Weiran Yao, <b>Jianguo Zhang</b><font color=#808080 >, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</font>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="">üèÜ Hugging Face Daily Papers</a></font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2308.05960.pdf">[Paper]</a></font> / <font color=#527bbd><a href="https://github.com/salesforce/BOLAA">[GitHub]</a></font> 
</li>	
	
<li>
	<b>DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang*</b><font color=#808080 >, Kun Qian*, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Silvio Savarese, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2024.eacl.org/">üèÜ Top 10 Trending Hugging Face Datasets in Sep. 2023 & Findings of EACL 2024</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2307.10172.pdf">[Paper]</a></font> / <font color=#527bbd><a href="https://github.com/salesforce/DialogStudio">[GitHub]</a></font> / <font color=#527bbd><a href="https://huggingface.co/datasets/Salesforce/dialogstudio">[HuggingFace]</a></font> 
</li>	

<li>
	<b>Retroformer: Retrospective large language agents with policy gradient optimization</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, <b>Jianguo Zhang</b><font color=#808080 >, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd> <a href="https://iclr.cc/">üèÜ ICLR 2024 - Spotlight + Hugging Face #1 Paper of the Day</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2308.02151.pdf">[Paper]</a></font> / <font color=#527bbd><a href="https://github.com/weirayao/Retroformer">[GitHub]</a></font> 
</li>	

<li>
	<b>Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Stephen Roller, Kun Qian, Zhiwei Liu, Rui Meng, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2023.sigdial.org/">SIGDIAL 2023</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://aclanthology.org/2023.sigdial-1.47/">[Paper]</a></font> 
</li>	

<li>
	<b>Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-Oriented Dialogue Systems</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Yihao Feng, Shentao Yang, Shujian Zhang, <b>Jianguo Zhang</b><font color=#808080 >, Caiming Xiong, Mingyuan Zhou, Huan Wang</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href=https://iclr.cc/Conferences/2023/">ICLR 2023</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2302.10342.pdf">[Paper]</a></font> 
</li>	
	
<li>
	<b>Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ziwei Fan, Zhiwei Liu, Shelby Heinecke, <b>Jianguo Zhang</b><font color=#808080 >, Huan Wang, Caiming Xiong, Philip S Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://uobevents.eventsair.com/cikm2023/">CIKM 2023</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://dl.acm.org/doi/pdf/10.1145/3583780.3615110">[Paper]</a></font> 
</li>	
	
<li>
	<b>Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Yao Wan, Zhiwei Liu, Ye Liu, Caiming Xiong, Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://sites.google.com/view/4thnlp4convai/home">ACL 2022</a> Workshop on NLP for Conversational AI</font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2106.04564.pdf">[Paper]</a></font> <font color=#527bbd><a href="https://github.com/jianguoz/Few-Shot-Intent-Detection">[GitHub]</a></font> 
</li>	
		
<li>
	<b>NaturalCC: An Open-Source Toolkit for Code Intelligence</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Yao Wan, Yang He, Zhangqian Bi, </font> <b>Jianguo Zhang</b><font color=#808080 >, Yulei Sui, Hongyu Zhang, Kazuma Hashimoto, Hai Jin, Guandong Xu, Caiming Xiong and Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://conf.researchr.org/track/icse-2022/icse-2022-demo---demonstrations"> ICSE 2022 Demo Track</a></font>  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/CGCL-codes/naturalcc" >Toolkit</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> 
</li>

<li>
	<b>Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang and Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2021.emnlp.org/">EMNLP 2021</a></font> (Oral)</font>
<!-- 	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2021.emnlp.org/">EMNLP 2021</a></font> short paper <font color='red'>(</font><font color='red'>Oral</font><font color='red'>)</font> -->
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://aclanthology.org/2021.emnlp-main.144.pdf">[Paper]</a></font> 
</li>

<li>
	<b>HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization</b>  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Ye Liu, </font> <b>Jianguo Zhang</b><font color=#808080 >, Yao Wan, Congying Xia, Lifang He and Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2021.emnlp.org/">EMNLP 2021</a></font>(Oral)</font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://aclanthology.org/2021.emnlp-main.13.pdf">[Paper]</a></font> 
</li>
	
<li>
	<b>Enriching Non-Autoregressive Transformer with Syntactic and Semantic Structures for Neural Machine Translation</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Ye Liu, Yao Wan, </font> <b>Jianguo Zhang</b><font color=#808080 >, Wenting Zhao, Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2021.eacl.org/">EACL 2021</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://www.aclweb.org/anthology/2021.eacl-main.105/">[Paper]</a></font> 
</li>
	
<li>
	<b>Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Wenhao Liu, Chien-Sheng Wu, Yao Wan, Philip S Yu, Richard Socher, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2020.emnlp.org/">EMNLP 2020</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2010.13009">[Paper]</a></font> / <font color=#527bbd><a href="https://drive.google.com/file/d/19l6XQ6YO6rNILPkTw890qrTHfOTEYv3o/view?usp=sharing">[Slide]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://slideslive.com/38939288/discriminative-nearest-neighbor-fewshot-intent-detection-by-transferring-natural-language-inference" >Video</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/salesforce/DNNC-few-shot-intent" >Code</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> 
</li>
<li>
	<b>Find or Classify? Dual Strategy for Slot-Value Predictions on Multi-Domain Dialog State Tracking</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wan, Philip S Yu, Richard Socher, Caiming Xiong </font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://sites.google.com/view/starsem2020/">*SEM 2020</a></font>(Oral)</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://www.aclweb.org/anthology/2020.starsem-1.17/">[Paper]</a></font> / <font color=#527bbd><a href="https://drive.google.com/file/d/1iBuZT13A3_SAtKqlP6OGpzi4FdzQRGRl/view?usp=sharing">[Slide]</a></font> / <font color=#527bbd><a href="xxxxx">[Video]</a></font>  
</li>
<li>
	<b>MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta,</font> <b>Jianguo Zhang</b><font color=#808080 >, Jindong Chen</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://sites.google.com/view/2ndnlp4convai/">ACL 2020</a> Workshop on NLP for Conversational AI</font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2007.12720.pdf">[Paper]</a></font> / <a href="xxxxx">[</a></font><font ><a href="https://github.com/budzianowski/multiwoz" >Dataset</a> <font color=#527bbd><font color=#527bbd><a href="xxxxx">]</a><a href="https://slideslive.com/38929641/multiwoz-22-a-dialogue-dataset-with-additional-annotation-corrections-and-state-tracking-baselines">[Video]</a></font> </font><font color=#527bbd></font></font> 
</li>
<li>
	<b>Multi-Modal Generative Adversarial Network for Short Product Title Generation in Mobile E-Commerce</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Pengcheng Zou, Zhao Li, Yao Wan, Xiuming Pan, Yu Gong, Philip S Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://naacl.org/naacl-hlt-2019/">NAACL-HLT 2019</a></font> (Oral) </font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://www.aclweb.org/anthology/N19-2009/">[Paper]</a></font>  
<!-- 		<font color='red'>(</font><font color='red'>Oral</font><font color='red'>)</font> -->
<!-- 		<font color='red'>(</font><font color='red'>Oral Presentation</font><font color='red'>)</font> -->
	
<!--       <button style="background-color:red"; title="Natural Language Processing">NLP</button> -->
</li>

<li>
	<b>Product Title Refinement via Multi-Modal Generative Adversarial Learning</b> &nbsp;  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Pengcheng Zou, Zhao Li, Yao Wan, Ye Liu, Xiuming Pan, Yu Gong, Philip S Yu</font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://nips.cc/Conferences/2018/">NeurIPS 2018</a> Workshop on ViGIL</font>
		<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/1811.04498">[Paper]</a></font>
<!-- 		(A short arXiv version of the NAACL-HLT 2019 paper and no Proceedings for the Workshop) -->
</li>
<li>
	<b>Layerwise Perturbation-Based Adversarial Training for Hard Drive Health Degree Prediction</b> &nbsp;  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang*</b><font color=#808080 >, Ji Wang*, Lifang He, Zhao Li, Philip S Yu (* indicates equal contribution)</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="http://icdm2018.org/">ICDM 2018</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/1809.04188.pdf">[Paper]</a></font>
</li>
<li>
	<b>Not just privacy: Improving performance of private deep learning in mobile cloud</b> &nbsp;  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Ji Wang*,</font> <b>Jianguo Zhang*</b> <font color=#808080 >, Weidong Bao, Xiaomin Zhu, Bokai Cao, Philip S Yu (* indicates equal contribution)</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://www.kdd.org/kdd2018/">KDD 2018</a></font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/1809.03428.pdf">[Paper]</a></font>
</li>


</ul>



 <br><br>

<div id="experience">
<h2>Experience</h2>

<table id="tbPublications" width="100%"> 
<tr>
        <td>
            <p><a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a>, New York, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp June 2021 - Sep. 2021</p> 
<!--  	<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019, &nbsp Summer 2019</p> --> 
          <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentor:  <a href="https://stephenroller.com/
	">Stephen Roller</a>
<!--                  ,<a href="https://david-yoon.github.io/">David Seunghyun Yoon</a>, <a href="https://research.adobe.com/person/xiang-chen/">Xiang Chen</a> and <a href="https://research.adobe.com/person/walter-chang/">Walter Chang</a>. -->
            </p>  
<!-- 	     <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp (I become an NLPer from here!) -->
        </td> 
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/fair-3.png" height=58px></p>
        </td>
  </tr> 

	<tr>
        <td>
            <p><a href="https://research.adobe.com/">Adobe Research</a>, San Jose, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Mar. 2021 - May 2021</p> 
<!--  	<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019, &nbsp Summer 2019</p> --> 
          <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Collaborators:  <a href="https://sites.google.com/site/trungbuistanford/Home
	">Trung Bui</a>,
                 <a href="https://david-yoon.github.io/">David Seunghyun Yoon</a> and <a href="https://research.adobe.com/person/walter-chang/">Walter Chang</a>.
            </p> 
<!-- 	     <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp (I become an NLPer from here!) -->
        </td> 
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/Adobe-Logo.png" height=58px></p>
        </td>
    </tr> 

	
<tr>
        <td>
            <p><a href="https://einstein.ai/">Salesforce Research</a>, Palo Alto, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019 - Summer 2020</p> 
<!--  	<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019, &nbsp Summer 2019</p> --> 
             <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors:  <a href="http://www.logos.t.u-tokyo.ac.jp/~hassy/
	">Kazuma Hashimoto</a>,
                 <a href="http://cmxiong.com/">Caiming Xiong</a>, <a href="https://jasonwu0731.github.io/">Chien-Sheng Wu</a> and <a href="https://www.socher.org/">Richard Socher</a>.
            </p> 
	     <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp (I become an NLPer from here!)
        </td>
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/salesforce-research.png" height=80px></p>
        </td>
    </tr> 

    <tr>
        <td>
            <p><a href="https://damo.alibaba.com/">Alibaba Group</a>, Hangzhou, China</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Summer 2018</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors: 
                <a href="https://www.linkedin.com/in/pengcheng-zou-765640105/?originalSubdomain=cn">Pengcheng Zou</a> and <a href="https://sites.google.com/view/zhaoli">Zhao Li</a>.
            </p> 
        </td>
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/alibaba-group.png" height=76px></p> 
        </td>
    </tr>
</table>
</div>

<h2>Service</h2> 
<li><b>Mentoring Interns at Salesforce</b>:  <a href="https://zixianma.github.io/">Zixian Ma (2024)</a>,  <a href="https://yangliangwei.github.io/">Liangwei Yang (2023)</a>,  <a href="https://www.linkedin.com/in/ziwei-fan-773513ba/">Ziwei Fan (2022)</a> </li>
<li><b>Program Committee/Reviewer</b>: ACL Rolling Review 2021-pres, EMNLP 2021-Pres, NeurIPS 2023-Pres, EACL 2023, ACL 2021/2022, NAACL 2021, IEEE/ACM TASLP 2020-Pres, and others. </li>
<li><b>Session Chair</b>: COLING 2020 (industry track: Dialogue).</li>
<li><b>Senior Program Committee</b>: CIKM 2023-Pres.</li>
<br><br> 
	
<!-- <h2>Misc</h2> 
I am a <a href="https://github.com/jianguoz/Conversational-AI/blob/master/misc/images_canon/readme.md"> photographer </a> (Keep learning), guitar lover, pet lover (My cat name is <a href="https://github.com/jianguoz/Conversational-AI/blob/master/misc/Maomao/readme.md">Maomao</a>), Chinese literature (Already read and forget many books) and basketball (Go Lakers, Metamind Lebron) fan.  
<!-- <a href="https://info.flagcounter.com/BimD"><img src="https://s04.flagcounter.com/mini/BimD/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a> <a href="https://github.com/jianguoz/Conversational-AI/blob/master/misc/images_canon/readme.md"> -->
<!-- <br><br>   -->

<h2>This website is under construction</h2>

<!--  <a href="https://info.flagcounter.com/BimD"><img src="https://s04.flagcounter.com/mini/BimD/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>  -->
</div>
</body></html>
